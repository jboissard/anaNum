%!TEX root = /Users/Johan/Documents/University/NumericalAnalysis/LaTeX/ananumessai.tex
\chapter{Système linéaire}
\section{Formulation du problème}
On appelle système linéaire d’ordre n (n entier positif), une expression de la 
forme 
\begin{eqnarray}
	Ax = b,
\end{eqnarray}
où $A = (a_{ij})$ est une matrice de taille $n \times n$ donnée, $b = (b_j)$ est un vecteur 
colonne également donné et $x = (x_j)$ est le vecteur des inconnues du système.

La matrice $A$ est dite régulière (non singulière) si $det A \neq 0$.

On a l’existence et l’unicité de la solution $x$ (pour n’importe quel vecteur $b$ donné) si et 
seulement si la matrice associée au système linéaire est régulière.

Théoriquement, si $A$ est régulière, la solution peut être obtenue par la \emph{formule de Cramer}.
%TODO: mettre ici formule de Cramer comme rapel
\section{Méthode de factorisation LU}
\textbf{Idée :} Soit $A$ une matrice singulière $n\times n$. Supposons qu’il existe une matrice triangulaire supérieure $U$ et une matrice triangulaire 
inférieure $L$, telles que :  $A = LU$ qui s’appelle une factorisation LU de $A$.
 
Si on connaît LU de $A$, résoudre $Ax = b$ équivaut à résoudre 2 systèmes à matrice triangulaire : 
\begin{eqnarray}
	Ax=b 
	\Leftrightarrow 
	LUx=b 
	\Leftrightarrow
	\begin{cases}
		Ly=b
		\\
		Ux=y
	\end{cases}
\end{eqnarray}
On peut calculer facilement les solutions de ces deux systèmes:
\begin{itemize}
	\item on utilise une substitution progressive pour calculer $Ly = b$ (ordre $n^2$ flops),
	\item on utilisee une substitution rétrograde pour calculer $Ux = y$ (ordre $n^2$ flops)
\end{itemize}
Cependant, il faut d’abord déterminer (s’il est possible) les matrices $L$ et $U$ (ce 
qui requiert un ordre de $\frac{2n^3}{3}$ flops). 

\section{Systèmes triangulaires}
\textbf{Définition:} Une matrice $A$ est 
\begin{itemize}
	\item \emph{triangulaire supérieure} si : $a_{ij}=0$ $\forall i,j : 1\leq j\leq i \leq n$
	\item \emph{triangulaire inférieure} si : $a_{ij}=0$ $\forall i,j : 1\leq i\leq j \leq n$
\end{itemize}

\begin{enumerate}
	\item \textbf{Méthode de substitution progressive:}
	Si $A$ est régulière et \emph{triangulaire inférieure} on a:
	\begin{eqnarray}
		\begin{cases}
			a_{11}x_1=b_1
			\\
			a_{21}x_1+a_{22}x_2=b_2
			\\
			\vdots
			\\
			a_{n1}x_1+a_{n2}x_2+\dots+a_{nn}x_n=b_n
		\end{cases}
	\end{eqnarray}
	et donc
	\begin{eqnarray}
		x_1=b_1/a_{11}
		\\
		x_i=\frac{1}{a_{ii}}\left(b_i-\sum_{j=1}^na_{ij}x_j\right)
	\end{eqnarray}
	où $i = 2,3,\dots,n$.
	\item \textbf{Méthode de substitution rétrograde:}
	Si $A$ est régulière et \emph{triangulaire inférieure} on a:
	\begin{eqnarray}
		\begin{cases}
			a_{11}x_1+\dots+a_{1,n-1}x_{n-1}+a_{1n}x_n=b_1
			\\
			\vdots
			\\
			a_{n-1,n-1}x_{n-1}+a_{n-1,n}x_n=b_{n-1}
			\\
			a_{nn}x_n=b_n
		\end{cases}
	\end{eqnarray}
	et donc
	\begin{eqnarray}
		x_n=b_n/a_{nn}
		\\
		x_i=\frac{1}{a_{ii}}\left(b_i-\sum_{j=i+1}^na_{ij}x_j\right)
	\end{eqnarray}
	où $i = n-1,n-2,\dots,2,1$.
\end{enumerate}

Pour déterminer la factorisation LU de la matrice $A$ de dimension $n$ quelconque, on appliquera la méthode suivante : 
\begin{itemize}
	\item Les éléments $L$ et $U$ satisfont le système non-linéaire:
	\begin{eqnarray}
		\label{eq:sysLU}
		\sum_{r=1}^{min(i,j)}l_{ir}u_{rj}=a_{ij},&i,j=1,\dots,n,
	\end{eqnarray} 
	\item le système (\ref{eq:sysLU}) possède $n^2$ équations avec $n^2+n$ inconnues, on peut éliminer $n$ inconnues si on pose les $n$ termes diagonaux de $L$ égaux à 1 
	\item voir méthode de Gauss ci-dessous pour calculer efficacement les $L$ et $U$ nécessaires.
\end{itemize}
\section{Méthode d'élimination de Gauss}
\textbf{Principe :} on désire transformer le système $Ax = b$ en un système modifié de solution identique : $Ux=\hat b$
où $U$ est une matrice triangulaire supérieure.

Considérons une matrice inversible $A$ de taille $n\times n$ dont le terme diagonal $a_{11}$ est supposé $\neq 0$.
 
On pose $A^{(1)} = A$ et $b^{(1)} = b$. On introduit le multiplicateur suivant :
\begin{eqnarray}
	l_{i1}=\frac{a_{i1}^{(1)}}{a_{11}^{(1)}},&i=2,3,\dots,n,
\end{eqnarray}
où 
\begin{eqnarray*}
	A^{(1)}=
	\begin{pmatrix}
		a_{11}^{(1)} & \cdots & a_{1n}^{(1)} 
		\\
		\vdots
		\\
		a_{i1}^{(1)} & \cdots & a_{in}^{(1)} 
		\\
		\vdots
		\\
		a_{n1}^{(1)} & \cdots & a_{nn}^{(1)} 
	\end{pmatrix}
\end{eqnarray*}
\section{Factorisation de Choleski}
\section{Méthodes itératives pour systèmes linéaires}
\begin{eqnarray*}
	A\overrightarrow{x}=\overrightarrow b
\end{eqnarray*}
t.q. $lim_{k\rightarrow\infty}\overrightarrow x^{(k)}=\overrightarrow x$

Une stratégie générale\\
Soit $B$ une matrice $n x n$ donnée et $\overrightarrow g$ un vecteur $(n)$ donné. Soit $x_0$ un vecteur donné, on considère la méthode itérative suivante 
\begin{eqnarray}
	\overrightarrow x^{(k+1)}=B\overrightarrow x ^{(k)}+\overrightarrow g
\end{eqnarray}

Supposons que $lim_{k\rightarrow\infty}\overrightarrow x^{(k)}=\overrightarrow x$ alors
$\overrightarrow x^{(k+1)}=B\overrightarrow x ^{(k)}+\overrightarrow g$ tend vers 
\begin{eqnarray}
	\overrightarrow x=B\overrightarrow x+\overrightarrow g
\end{eqnarray} 
quand $k\rightarrow\infty$
\\On en déduit
\begin{eqnarray*}
	\overrightarrow g =(\overrightarrow x -B\overrightarrow x)=(I-B)\overrightarrow x=(I-B)A^{-1}\overrightarrow b
\end{eqnarray*}
	

\textbf{Conclusion}
Dans le schéma (1), $B$ peut être choisi arbitrairement mais $\overrightarrow g$ doit satisfaire la relation $\overrightarrow g=(I-B)A^{-1}\overrightarrow b$
\\Critère e choix de $B$
\\Considérons (1) et (2)
\begin{eqnarray}
	\overrightarrow e^{(k+1)}=\overrightarrow{x}^{(k+1)}-\overrightarrow x=B\overrightarrow x{(k)}+\overrightarrow g-B\overrightarrow x - \overrightarrow g=B(\overrightarrow x^{(k)}-\overrightarrow x)=B\overrightarrow e^{(k)}=\overrightarrow e^{(k+1)}=B^{k+1}(\overrightarrow x^{(o)}-\overrightarrow x)
\end{eqnarray}
$\overrightarrow e{(0)}$ est l'erreur initiale, $\overrightarrow x^{(0)}$ est le vecteur initiale \underline{arbitraire} et donc $\overrightarrow e{(0)}$ est aussi un vecteur arbitraire.
\textbf{Défnition} On dit que la méthode itérative (1) est convergente si $lim_{k\rightarrow\infty}\overrightarrow x^{(k)}=\overrightarrow x$ $\forall\overrightarrow x^{(0)}$ c'est-à-dire
\begin{eqnarray*}
	lim_{k\rightarrow\infty}(\overrightarrow x^{(k)}-\overrightarrow x)=lim_{k\rightarrow\infty}\overrightarrow e^{(k)}=0
\end{eqnarray*}
$\rho(B)< 1$: rayon spectral
\\
\textbf{Conclusion} La méthode (1) est convergente $\Leftrightarrow \rho(B)< 1$
\textbf{Quelques exemples de méthodes itératives}
\underline{Méthode de Jacobi (J)}
\begin{eqnarray}
	x_i^{(k+1)}=\frac{1}{a_{ii}}\left[b_i-\sum_{j=1}^{n}a_{ij}x_{j}^{(k)}\right]
\end{eqnarray}
où $\overrightarrow g=D^{-1}\overrightarrow b$ et $B_J=I-D^{-1}A$. La matrice $D$ est défini comme suit:
\begin{eqnarray*}
	D=\begin{pmatrix} a_1    & 0      & \ldots & 0      \\ 0      & a_2    & \ddots & \vdots \\ \vdots & \ddots & \ddots & 0      \\ 0      & \ldots & 0      & a_k \end{pmatrix}
\end{eqnarray*}
où les $a_{ii}$ sont les coefficients diagonales de la matrice $A$.
\\
\textbf{Conclusion} La méthode de Jabi est un cas particulier de (1)
\\\\
\textbf{Méthode de Gauss-Seidel (GS)}
\\
On définit une matrice triangulaire inférieure $L$ t.q. $A=L+(A-L)$
\\On a la méthode itérative suivante
\begin{eqnarray}
	\overrightarrow x^{(k+1)}=(I-L^{-1}A)\overrightarrow x^{(k)}+L^{-1}\overrightarrow{b}
\end{eqnarray}
où
\begin{eqnarray*}
	B_{GS}=(I-L^{-1}A)
	\\
	\overrightarrow g = L^{-1}\overrightarrow{b}
\end{eqnarray*}
\textbf{Conclusion} La méthode de Gauss-Siedel est également un cas particulier de (1)
\\
\underline{Analyse de convergence pour J et GS}
On cherche des conditions sur $A$ afin que 
\\$\rho(B_J)<1\Rightarrow J$ converge
\\$\rho(B_{GS})<1\Rightarrow GS$ converge
\\\underline{Proposition 1}
\\
Si $A$ est une matrice à \underline{dominance diagonale stricte} alors $J$ et $GS$ convergent.
\\
Pour quune matrice soit à dominance diagonale stricte, il faut que la valeur absolue du coefficient de la diagonale soit supérieure à la somme des valeurs absolues des coefficients sur la même ligne. Exemple:
$
	\begin{pmatrix} 
		-5   & 1      & 2       
	\\ 3      & 8    & -4
	\\ 0 & -2 & 3  
 \end{pmatrix}$, dans ce cas on a $|-5|>|1|+|2|$,$|8|>|3|+|-4|$ et $|3|>|-2|+|0|$
\\\\
\textbf{Proposition 2} Si la matrice est symétrique et définie positiviement alors la méthode $GS$ converge.
\\Matrice symétrique $\Leftrightarrow A=A^T\Leftrightarrow a_{ij}=a_{ji}$
\\Matrice défnie positivement $\Leftrightarrow \lambda_i>0 \forall i$
