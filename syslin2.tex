%!TEX root = /Users/Johan/Documents/University/NumericalAnalysis/LaTeX/ananum.tex
\chapter{Système linéaire}
\section{Formulation du problème}
On appelle système linéaire d’ordre n (n entier positif), une expression de la 
forme 
\begin{eqnarray}
	Ax = b,
\end{eqnarray}
où $A = (a_{ij})$ est une matrice de taille $n \times n$ donnée, $b = (b_j)$ est un vecteur 
colonne également donné et $x = (x_j)$ est le vecteur des inconnues du système.

La matrice $A$ est dite régulière (non singulière) si $det A \neq 0$.

On a l’existence et l’unicité de la solution $x$ (pour n’importe quel vecteur $b$ donné) si et 
seulement si la matrice associée au système linéaire est régulière.

Théoriquement, si $A$ est régulière, la solution peut être obtenue par la \emph{formule de Cramer}
\begin{eqnarray}
	x_i=\frac{det(B_i)}{det(A)}
\end{eqnarray}
où
\begin{eqnarray*}
	B_i=
	\begin{pmatrix}
		a_{11} & \cdots & b_1 & \cdots & a_{1n}
		\\
		a_{21} & \cdots & b_2 & \cdots & a_{2n}
		\\
		\vdots & & & & \vdots
		\\
		a_{n1} & \cdots & b_n & \cdots & a_{nn}
	\end{pmatrix}.
\end{eqnarray*}
Le vecteur $b$ est inséré à la place de la $i^{\text{ème}}$ colonne dans la matrice $A$.
\section{Méthode de factorisation LU}
\textbf{Idée :} Soit $A$ une matrice singulière $n\times n$. Supposons qu’il existe une matrice triangulaire supérieure $U$ et une matrice triangulaire 
inférieure $L$, telles que :  $A = LU$ qui s’appelle une factorisation LU de $A$.
 
Si on connaît LU de $A$, résoudre $Ax = b$ équivaut à résoudre 2 systèmes à matrice triangulaire : 
\begin{eqnarray}
	Ax=b 
	\Leftrightarrow 
	LUx=b 
	\Leftrightarrow
	\begin{cases}
		Ly=b
		\\
		Ux=y
	\end{cases}
\end{eqnarray}
On peut calculer facilement les solutions de ces deux systèmes:
\begin{itemize}
	\item on utilise une substitution progressive pour calculer $Ly = b$ (ordre $n^2$ flops),
	\item on utilisee une substitution rétrograde pour calculer $Ux = y$ (ordre $n^2$ flops)
\end{itemize}
Cependant, il faut d’abord déterminer (s’il est possible) les matrices $L$ et $U$ (ce 
qui requiert un ordre de $\frac{2n^3}{3}$ flops). 

\section{Systèmes triangulaires}
\textbf{Définition:} Une matrice $A$ est 
\begin{itemize}
	\item \emph{triangulaire supérieure} si : $a_{ij}=0$ $\forall i,j : 1\leq j\leq i \leq n$
	\item \emph{triangulaire inférieure} si : $a_{ij}=0$ $\forall i,j : 1\leq i\leq j \leq n$
\end{itemize}
On a également la propriété suivante:
\begin{eqnarray}
	\det A=\prod_{i=1}^{n}\lambda_i(A)=\prod_{i=1}^na_{ii}
\end{eqnarray}
on en déduit immédiatement que $a_{ii}\neq0$ $\forall i\in\mathbb N^*$.
\begin{enumerate}
	\item \textbf{Méthode de substitution progressive:}
	Si $A$ est régulière et \emph{triangulaire inférieure} on a:
	\begin{eqnarray}
		\begin{cases}
			a_{11}x_1=b_1
			\\
			a_{21}x_1+a_{22}x_2=b_2
			\\
			\vdots
			\\
			a_{n1}x_1+a_{n2}x_2+\dots+a_{nn}x_n=b_n
		\end{cases}
	\end{eqnarray}
	et donc
	\begin{eqnarray}
		x_1=b_1/a_{11}
		\\
		x_i=\frac{1}{a_{ii}}\left(b_i-\sum_{j=1}^na_{ij}x_j\right)
	\end{eqnarray}
	où $i = 2,3,\dots,n$.
	\item \textbf{Méthode de substitution rétrograde:}
	Si $A$ est régulière et \emph{triangulaire inférieure} on a:
	\begin{eqnarray}
		\begin{cases}
			a_{11}x_1+\dots+a_{1,n-1}x_{n-1}+a_{1n}x_n=b_1
			\\
			\vdots
			\\
			a_{n-1,n-1}x_{n-1}+a_{n-1,n}x_n=b_{n-1}
			\\
			a_{nn}x_n=b_n
		\end{cases}
	\end{eqnarray}
	et donc
	\begin{eqnarray}
		x_n=b_n/a_{nn}
		\\
		x_i=\frac{1}{a_{ii}}\left(b_i-\sum_{j=i+1}^na_{ij}x_j\right)
	\end{eqnarray}
	où $i = n-1,n-2,\dots,2,1$.
\end{enumerate}

\subsection{matrice de permutation}
Si ontrouve une factorisation du type
\begin{eqnarray}
	PA = LU
\end{eqnarray}
le vecteur $x$, solution du système $Ax=b$, vérifie également
\begin{eqnarray}
	PAx=\tilde b
\end{eqnarray}
où $\tilde b =Pb$. Ce deuxième système s'écrit
\begin{eqnarray}
	LUx=\tilde b
\end{eqnarray}
Donx, comme auparavant, on le résout par $Ly=\tilde b$ et $Ux=y$
%2eme partie
\section{Méthodes itératives pour systèmes linéaires}

\subsection{Idée:}
\begin{eqnarray}
	x=\lim_{k\rightarrow\infty}x^{(k)}
	\\
	x^{k+1}=Bx^{(k)}+g
	\\
	x=Bx+g
\end{eqnarray}
on trouve que $g=(I-B)A^{-1}b$
\subsection{Erreur}
\begin{eqnarray}
	e^{(k)}=x-x^{(k)}=Be^{(k-1)}=B^ke^{(0)}&k=0,1,\dots
	\\
	\lim_{k\rightarrow\infty}e^{(k)}=0&\forall e^{(0)}\Leftrightarrow\rho(B)<1
\end{eqnarray}
où $\rho(B)=\max_i|\lambda_i(B)|$ est le rayon spectral de la matrice $B$.
\section{Jacobi}
\begin{eqnarray}
	x^{(k+1)}=B_Jx^{(k)}+g_j
\end{eqnarray} 
où $B_J=I-D^{-1}A$ et $g_J=D^{-1}b$ et $D=
	\begin{cases}
		0&i\neq j
		\\
		a_{ij}&i=j
	\end{cases}$
\section{Gauss-Siedel}
	\begin{eqnarray}
		B_{GS}=I-L^{-1}A
		\\
		L=
			\begin{cases}
				0&i< j
				\\
				a_{ij}&i\geq j
			\end{cases}
	\end{eqnarray}
	On peut également écrire de la façon suivante
	\begin{eqnarray*}
		E=
			\begin{cases}
				-a_{ij}&i> j
				\\
				0&i\leq j
			\end{cases}
			\\
		F=
			\begin{cases}
				-a_{if}&i< j
				\\
				0&i\geq j
			\end{cases}
			\\
			A=D-(E+F)
			\\
			L=D-E
			\\
			B_{GS}=(D-E)^{-1}(D-E-A)
	\end{eqnarray*}
	
\section{Convergence}
\begin{itemize}
	\item si $A$ diagonale dominante stricte ($a_{ii}>\sum_{i\neq j}|a_{ij}|$), alors GS et J convergent
	\item si $A$ régulière et tridiagonale (trois diagonales au centre symétriques) et $a_{ii}\neq0$ alors GS et J sont soit divergent soit convergent, dans le dernier cas on a: $\rho(B_{GS})^2=\rho(B_J)$
	%exemple dune matric tridiagonale , mais prend trop de place
	%\begin{pmatrix}
	%a_{11}& a_{21} & 0 & 0 &\cdots & 0
	%\\
	%a_{21} & a_{22} & a_{32} & 0 & \cdots & 0
	%\\
	%0 & a_{32} & a_{33}	& a_{43} & \cdots & 0
	%\\
	%\vdots & & & & & \vdots
	%\\
	%0 & 0 & 0 & 0& a_{n,n-1} &a_{nn}
	%\end{pmatrix}

	\item si $A$ symétrique et définie positive ($\lambda_i\geq0$), alors GS converge
\end{itemize}

\section{Richardson}
\begin{eqnarray}
	x^{k+1}=x^{(k)}+\alpha_k r^{(k)}
\end{eqnarray}
où $r^{(k)}=b-Ax^{(k)}$ est le résidu, $\alpha_k$ un coefficient.

\begin{itemize}
	\item Si $\alpha_k=\text{const}$, méthode statique
	\begin{eqnarray}
		\alpha_k=\alpha_{opt}=\frac{2}{\lambda_{max}+\lambda_{min}}
	\end{eqnarray}
	\item Sinon méthode dynamique: 
	\begin{eqnarray}
		\alpha_k=\frac{(r^{(k)},r^{(k)})}{(Ar^{(k)},r^{(k)})},
	\end{eqnarray}
	où $(x,y)=\sqrt{\sum_{i=1}^nx_iy_i}$ est le produit scalaire.
\end{itemize} 

\subsection{Convergence}
\begin{eqnarray}
	||x^{(k)}-x||_A\leq\left(\frac{K(A)-1}{K(A)+1}\right)^k||x^{(0)}-x||_A&k\geq0
\end{eqnarray}
où $K(A)$ est le conditionnement de la matrice $A$ défini par
\begin{eqnarray}
	K(A)=\frac{\lambda_{max}(A)}{\lambda_{min}(A)}
\end{eqnarray}

\subsection{Préconditionneur}
	On peut ajouter une matrice $P$ pour améliorer la vitesse de convergence de la méthode, on obtient
	\begin{eqnarray}
		P(x^{(k+1)}-x^{(k)})=\alpha_kr^{(k)}
	\end{eqnarray}
On a les $\alpha_k$ suivants:
\begin{eqnarray}
	\alpha_{opt}=\frac{2}{\lambda_{min}(P^{-1}A)+\lambda_{max}(P^{-1}A)}
	\\
	\alpha_k=\frac{(r^{(k)},P^{-1}r^{(k)})}{(AP^{-1}r^{(k)},P^{-1}r^{(k)})}.
\end{eqnarray}
Et pour la convergence
\begin{eqnarray}
	||x^{(k)}-x||_A\leq\left(\frac{K(P^{(-1)}A)+1}{K(P^{(-1)}A)-1}\right)^k||x^{(0)}-x||_A&k\geq0
\end{eqnarray}
\underline{Remarques:} Si on remplace $P^{-1}$ par la matrice identité $I$ dans les formules précédentes, on retombe sur les formules originales.
